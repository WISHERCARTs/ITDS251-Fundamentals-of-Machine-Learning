{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a214a587",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17fabac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (400, 2) Labels (0/1) counts: [142 258]\n",
      "\n",
      "=== Test Results ===\n",
      "Accuracy: 0.89\n",
      "Confusion matrix [[TN FP],[FN TP]]:\n",
      " [[28  7]\n",
      " [ 4 61]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8750    0.8000    0.8358        35\n",
      "           1     0.8971    0.9385    0.9173        65\n",
      "\n",
      "    accuracy                         0.8900       100\n",
      "   macro avg     0.8860    0.8692    0.8766       100\n",
      "weighted avg     0.8893    0.8900    0.8888       100\n",
      "\n",
      "Intercept (b): -5.873737079819337\n",
      "Weights (w1, w2): [0.76603786 0.5501515 ]\n",
      "Feature order: [study_hours, sleep_hours]\n",
      "\n",
      "=== New samples ===\n",
      "Sample 1: study=2.0, sleep=4.0 -> class=0, P(y=1)=0.105\n",
      "Sample 2: study=8.0, sleep=6.0 -> class=1, P(y=1)=0.972\n",
      "Sample 3: study=5.0, sleep=8.0 -> class=1, P(y=1)=0.914\n"
     ]
    }
   ],
   "source": [
    "# Binary Classification Demo: Logistic Regression (Mock dataset)\n",
    "# - Pure Python + scikit-learn\n",
    "# - Prints accuracy + confusion matrix + learned weights\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Mock (synthetic) dataset\n",
    "# ----------------------------\n",
    "# We generate 2 numeric features:\n",
    "#   x1 = \"study_hours\"\n",
    "#   x2 = \"sleep_hours\"\n",
    "# Label rule (with noise):\n",
    "#   more study + decent sleep -> higher chance to pass (y=1)\n",
    "rng = np.random.default_rng(42)\n",
    "N = 400\n",
    "\n",
    "study_hours = rng.uniform(0, 10, size=N)   # 0..10\n",
    "sleep_hours = rng.uniform(3, 9, size=N)    # 3..9\n",
    "\n",
    "# Linear score + noise\n",
    "noise = rng.normal(0, 1.2, size=N)\n",
    "score = 1.1 * study_hours + 0.7 * sleep_hours - 8.0 + noise\n",
    "\n",
    "# Convert score -> probability via sigmoid, then sample label\n",
    "prob = 1.0 / (1.0 + np.exp(-score))\n",
    "y = (rng.uniform(0, 1, size=N) < prob).astype(int)\n",
    "\n",
    "# Feature matrix\n",
    "X = np.column_stack([study_hours, sleep_hours])\n",
    "\n",
    "print(\"Dataset shape:\", X.shape, \"Labels (0/1) counts:\", np.bincount(y))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train / Test split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Train Logistic Regression\n",
    "# ----------------------------\n",
    "# solver='liblinear' is great for small binary problems\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Evaluate\n",
    "# ----------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Test Results ===\")\n",
    "print(\"Accuracy:\", round(acc, 4))\n",
    "print(\"Confusion matrix [[TN FP],[FN TP]]:\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Show learned parameters\n",
    "# ----------------------------\n",
    "print(\"Intercept (b):\", model.intercept_[0])\n",
    "print(\"Weights (w1, w2):\", model.coef_[0])\n",
    "print(\"Feature order: [study_hours, sleep_hours]\")\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Predict a few new samples\n",
    "# ----------------------------\n",
    "new_X = np.array([\n",
    "    [2.0, 4.0],   # low study, low sleep\n",
    "    [8.0, 6.0],   # high study, ok sleep\n",
    "    [5.0, 8.0],   # mid study, high sleep\n",
    "], dtype=float)\n",
    "\n",
    "pred_class = model.predict(new_X)\n",
    "pred_prob = model.predict_proba(new_X)[:, 1]\n",
    "\n",
    "print(\"\\n=== New samples ===\")\n",
    "for i, (x, c, p) in enumerate(zip(new_X, pred_class, pred_prob), start=1):\n",
    "    print(f\"Sample {i}: study={x[0]:.1f}, sleep={x[1]:.1f} -> class={c}, P(y=1)={p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4bac3",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30d3da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (150, 4)\n",
      "Classes: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "====================\n",
      "MULTICLASS: setosa vs versicolor vs virginica\n",
      "====================\n",
      "Accuracy: 0.9737\n",
      "Confusion matrix:\n",
      " [[13  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 12]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa     1.0000    1.0000    1.0000        13\n",
      "  versicolor     1.0000    0.9231    0.9600        13\n",
      "   virginica     0.9231    1.0000    0.9600        12\n",
      "\n",
      "    accuracy                         0.9737        38\n",
      "   macro avg     0.9744    0.9744    0.9733        38\n",
      "weighted avg     0.9757    0.9737    0.9737        38\n",
      "\n",
      "Intercepts (b) per class: [  9.15004074   1.91037397 -11.06041471]\n",
      "Weights (rows=classes, cols=features):\n",
      "\n",
      "Class: setosa\n",
      "  sepal length (cm)         -0.4167\n",
      "  sepal width (cm)          +0.8667\n",
      "  petal length (cm)         -2.2962\n",
      "  petal width (cm)          -0.9919\n",
      "\n",
      "Class: versicolor\n",
      "  sepal length (cm)         +0.5389\n",
      "  sepal width (cm)          -0.3800\n",
      "  petal length (cm)         -0.2111\n",
      "  petal width (cm)          -0.7400\n",
      "\n",
      "Class: virginica\n",
      "  sepal length (cm)         -0.1222\n",
      "  sepal width (cm)          -0.4867\n",
      "  petal length (cm)         +2.5074\n",
      "  petal width (cm)          +1.7319\n",
      "\n",
      "Sample: [5.9 3.  5.1 1.8]\n",
      "Predicted class: virginica\n",
      "Class probabilities: {np.str_('setosa'): np.float64(0.0008), np.str_('versicolor'): np.float64(0.2452), np.str_('virginica'): np.float64(0.754)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Iris (Binary + Multiclass demo)\n",
    "# - Binary: setosa vs not-setosa\n",
    "# - Multiclass: all 3 classes\n",
    "# - Prints accuracy, confusion matrix, and learned weights\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load Iris\n",
    "# ----------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data                       # (150, 4)\n",
    "y_multi = iris.target               # 0=setosa, 1=versicolor, 2=virginica\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Classes:\", iris.target_names)\n",
    "\n",
    "# =========================================================\n",
    "# Multiclass Logistic Regression: 3-class classification\n",
    "# =========================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_multi, test_size=0.25, random_state=7, stratify=y_multi\n",
    ")\n",
    "\n",
    "# lbfgs supports multinomial well\n",
    "multi_model = LogisticRegression(\n",
    "    solver=\"lbfgs\",\n",
    "    multi_class=\"multinomial\",\n",
    "    max_iter=200,\n",
    "    random_state=7\n",
    ")\n",
    "multi_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = multi_model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(\"MULTICLASS: setosa vs versicolor vs virginica\")\n",
    "print(\"====================\")\n",
    "print(\"Accuracy:\", round(acc, 4))\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names, digits=4))\n",
    "\n",
    "print(\"Intercepts (b) per class:\", multi_model.intercept_)\n",
    "print(\"Weights (rows=classes, cols=features):\")\n",
    "for class_idx, class_name in enumerate(iris.target_names):\n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    for name, w in zip(feature_names, multi_model.coef_[class_idx]):\n",
    "        print(f\"  {name:25s} {w:+.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Predict a sample (optional)\n",
    "# ----------------------------\n",
    "sample = np.array([[5.9, 3.0, 5.1, 1.8]])  # one iris sample\n",
    "pred_class = multi_model.predict(sample)[0]\n",
    "pred_prob = multi_model.predict_proba(sample)[0]\n",
    "\n",
    "print(\"\\nSample:\", sample[0])\n",
    "print(\"Predicted class:\", iris.target_names[pred_class])\n",
    "print(\"Class probabilities:\", dict(zip(iris.target_names, np.round(pred_prob, 4))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ce9c87",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "--------------------------------------------\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c276ae",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c963f3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "LINEAR SVM (Multiclass): 3-class Iris\n",
      "====================\n",
      "Accuracy: 0.9474\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[13  0  0]\n",
      " [ 0 11  2]\n",
      " [ 0  0 12]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa     1.0000    1.0000    1.0000        13\n",
      "  versicolor     1.0000    0.8462    0.9167        13\n",
      "   virginica     0.8571    1.0000    0.9231        12\n",
      "\n",
      "    accuracy                         0.9474        38\n",
      "   macro avg     0.9524    0.9487    0.9466        38\n",
      "weighted avg     0.9549    0.9474    0.9472        38\n",
      "\n",
      "coef_ shape: (3, 4) (classes x features)\n",
      "intercept_ shape: (3,) (classes,)\n",
      "\n",
      "Class 'setosa' hyperplane (in SCALED feature space):\n",
      "  bias (b): -0.7997\n",
      "  sepal length (cm)         -0.2036\n",
      "  sepal width (cm)          +0.3788\n",
      "  petal length (cm)         -0.6622\n",
      "  petal width (cm)          -0.6772\n",
      "\n",
      "Class 'versicolor' hyperplane (in SCALED feature space):\n",
      "  bias (b): -0.376\n",
      "  sepal length (cm)         -0.0607\n",
      "  sepal width (cm)          -0.5040\n",
      "  petal length (cm)         +0.5945\n",
      "  petal width (cm)          -0.5695\n",
      "\n",
      "Class 'virginica' hyperplane (in SCALED feature space):\n",
      "  bias (b): -1.9236\n",
      "  sepal length (cm)         -0.3591\n",
      "  sepal width (cm)          -0.2799\n",
      "  petal length (cm)         +1.7761\n",
      "  petal width (cm)          +1.5327\n",
      "\n",
      "Sample: [5.9 3.  5.1 1.8]\n",
      "Predicted class: virginica\n",
      "Decision scores: {np.str_('setosa'): np.float64(-1.8662), np.str_('versicolor'): np.float64(-0.2959), np.str_('virginica'): np.float64(0.5844)}\n"
     ]
    }
   ],
   "source": [
    "# Multiclass (Multinomial-style) Classification with Linear SVM on Iris\n",
    "# StandardScaler is applied EXPLICITLY (separate from model)\n",
    "# NOTE: LinearSVC uses one-vs-rest (OvR) under the hood for multiclass.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load Iris (3 classes)\n",
    "# ----------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train / Test split\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Standard Scaling \n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Train Linear SVM (Multiclass)\n",
    "# ----------------------------\n",
    "svm = LinearSVC(C=1.0, random_state=7, max_iter=10000)  # multiclass handled internally\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Evaluate\n",
    "# ----------------------------\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n====================\")\n",
    "print(\"LINEAR SVM (Multiclass): 3-class Iris\")\n",
    "print(\"====================\")\n",
    "print(\"Accuracy:\", round(acc, 4))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Show learned parameters\n",
    "# ----------------------------\n",
    "# For multiclass LinearSVC:\n",
    "#   coef_.shape = (n_classes, n_features)\n",
    "#   intercept_.shape = (n_classes,)\n",
    "print(\"coef_ shape:\", svm.coef_.shape, \"(classes x features)\")\n",
    "print(\"intercept_ shape:\", svm.intercept_.shape, \"(classes,)\")\n",
    "\n",
    "for k, cname in enumerate(class_names):\n",
    "    print(f\"\\nClass '{cname}' hyperplane (in SCALED feature space):\")\n",
    "    print(\"  bias (b):\", round(svm.intercept_[k], 4))\n",
    "    for fname, w in zip(feature_names, svm.coef_[k]):\n",
    "        print(f\"  {fname:25s} {w:+.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict a new sample\n",
    "# ----------------------------\n",
    "sample = np.array([[5.9, 3.0, 5.1, 1.8]])\n",
    "sample_scaled = scaler.transform(sample)\n",
    "\n",
    "pred_class = svm.predict(sample_scaled)[0]\n",
    "scores = svm.decision_function(sample_scaled)[0]  # one score per class\n",
    "\n",
    "print(\"\\nSample:\", sample[0])\n",
    "print(\"Predicted class:\", class_names[pred_class])\n",
    "print(\"Decision scores:\", dict(zip(class_names, np.round(scores, 4))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d45a2b1",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a0df2",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ee0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CV results (on TRAIN only) ===\n",
      "\n",
      "Best k = 3  (mean CV acc = 0.9644)\n",
      "\n",
      "=== Test results ===\n",
      "Test accuracy: 0.9737\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[13  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 12]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa     1.0000    1.0000    1.0000        13\n",
      "  versicolor     1.0000    0.9231    0.9600        13\n",
      "   virginica     0.9231    1.0000    0.9600        12\n",
      "\n",
      "    accuracy                         0.9737        38\n",
      "   macro avg     0.9744    0.9744    0.9733        38\n",
      "weighted avg     0.9757    0.9737    0.9737        38\n",
      "\n",
      "\n",
      "=== New sample prediction ===\n",
      "Sample: [5.9 3.  5.1 1.8]\n",
      "Predicted class: virginica\n",
      "Probabilities: {np.str_('setosa'): np.float64(0.0), np.str_('versicolor'): np.float64(0.3333), np.str_('virginica'): np.float64(0.6667)}\n"
     ]
    }
   ],
   "source": [
    "# k-NN on Iris with CV to pick best k, then Train/Test + Predict\n",
    "# - Step 1: CV on training set to choose best k\n",
    "# - Step 2: Fit best model on training set\n",
    "# - Step 3: Evaluate on test set + predict a new sample\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load Iris\n",
    "# ----------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train/Test split (keep test untouched)\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Scale features (important for k-NN)\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Cross-validation to choose best k\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "k_list = list(range(1, 31, 2))  # odd k: 1,3,5,...,29\n",
    "mean_scores = []\n",
    "\n",
    "for k in k_list:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_s, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "best_idx = int(np.argmax(mean_scores))\n",
    "best_k = k_list[best_idx]\n",
    "best_cv_acc = mean_scores[best_idx]\n",
    "\n",
    "print(\"=== CV results (on TRAIN only) ===\")\n",
    "for k, m in zip(k_list, mean_scores):\n",
    "    print(f\"k={k:2d}  mean_CV_acc={m:.4f}\")\n",
    "print(f\"\\nBest k = {best_k}  (mean CV acc = {best_cv_acc:.4f})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Train best k-NN on full training set\n",
    "# ----------------------------\n",
    "best_model = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_model.fit(X_train_s, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Evaluate on test set\n",
    "# ----------------------------\n",
    "y_pred = best_model.predict(X_test_s)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "print(\"Test accuracy:\", round(test_acc, 4))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict a new sample\n",
    "# ----------------------------\n",
    "sample = np.array([[5.9, 3.0, 5.1, 1.8]])  # [sepal len, sepal wid, petal len, petal wid]\n",
    "sample_s = scaler.transform(sample)\n",
    "\n",
    "pred_class = best_model.predict(sample_s)[0]\n",
    "pred_prob = best_model.predict_proba(sample_s)[0]\n",
    "\n",
    "print(\"\\n=== New sample prediction ===\")\n",
    "print(\"Sample:\", sample[0])\n",
    "print(\"Predicted class:\", class_names[pred_class])\n",
    "print(\"Probabilities:\", dict(zip(class_names, np.round(pred_prob, 4))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66993f76",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4cd7e",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c26687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CV results (on TRAIN only) ===\n",
      "max_depth= 1  mean_CV_acc=0.6609\n",
      "max_depth= 2  mean_CV_acc=0.9289\n",
      "max_depth= 3  mean_CV_acc=0.9553\n",
      "max_depth= 4  mean_CV_acc=0.9553\n",
      "max_depth= 5  mean_CV_acc=0.9553\n",
      "max_depth= 6  mean_CV_acc=0.9553\n",
      "max_depth= 7  mean_CV_acc=0.9553\n",
      "max_depth= 8  mean_CV_acc=0.9553\n",
      "max_depth= 9  mean_CV_acc=0.9553\n",
      "max_depth=10  mean_CV_acc=0.9553\n",
      "\n",
      "Best max_depth = 3  (mean CV acc = 0.9553)\n",
      "\n",
      "=== Test results ===\n",
      "Test accuracy: 0.9737\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[13  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 12]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa     1.0000    1.0000    1.0000        13\n",
      "  versicolor     1.0000    0.9231    0.9600        13\n",
      "   virginica     0.9231    1.0000    0.9600        12\n",
      "\n",
      "    accuracy                         0.9737        38\n",
      "   macro avg     0.9744    0.9744    0.9733        38\n",
      "weighted avg     0.9757    0.9737    0.9737        38\n",
      "\n",
      "\n",
      "=== Best Tree rules (text) ===\n",
      "|--- petal length (cm) <= 2.45\n",
      "|   |--- class: 0\n",
      "|--- petal length (cm) >  2.45\n",
      "|   |--- petal width (cm) <= 1.75\n",
      "|   |   |--- petal length (cm) <= 4.95\n",
      "|   |   |   |--- class: 1\n",
      "|   |   |--- petal length (cm) >  4.95\n",
      "|   |   |   |--- class: 2\n",
      "|   |--- petal width (cm) >  1.75\n",
      "|   |   |--- petal length (cm) <= 4.85\n",
      "|   |   |   |--- class: 2\n",
      "|   |   |--- petal length (cm) >  4.85\n",
      "|   |   |   |--- class: 2\n",
      "\n",
      "\n",
      "=== New sample prediction ===\n",
      "Sample: [5.9 3.  5.1 1.8]\n",
      "Predicted class: virginica\n",
      "Probabilities: {np.str_('setosa'): np.float64(0.0), np.str_('versicolor'): np.float64(0.0), np.str_('virginica'): np.float64(1.0)}\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree on Iris with CV to choose best max_depth\n",
    "# - Step 1: CV on training set to pick best depth\n",
    "# - Step 2: Fit best tree on full training set\n",
    "# - Step 3: Evaluate on untouched test set + predict sample\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load Iris\n",
    "# ----------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train/Test split (keep test untouched)\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) CV to choose best max_depth (on TRAIN only)\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "depth_list = list(range(1, 11))  # 1..10\n",
    "mean_scores = []\n",
    "\n",
    "for d in depth_list:\n",
    "    tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=d, random_state=7)\n",
    "    scores = cross_val_score(tree, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "best_idx = int(np.argmax(mean_scores))\n",
    "best_depth = depth_list[best_idx]\n",
    "best_cv_acc = mean_scores[best_idx]\n",
    "\n",
    "print(\"=== CV results (on TRAIN only) ===\")\n",
    "for d, m in zip(depth_list, mean_scores):\n",
    "    print(f\"max_depth={d:2d}  mean_CV_acc={m:.4f}\")\n",
    "print(f\"\\nBest max_depth = {best_depth}  (mean CV acc = {best_cv_acc:.4f})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Train best Decision Tree on full training set\n",
    "# ----------------------------\n",
    "best_tree = DecisionTreeClassifier(criterion=\"gini\", max_depth=best_depth, random_state=7)\n",
    "best_tree.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Evaluate on test set\n",
    "# ----------------------------\n",
    "y_pred = best_tree.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "print(\"Test accuracy:\", round(acc, 4))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Show best tree rules (text)\n",
    "# ----------------------------\n",
    "print(\"\\n=== Best Tree rules (text) ===\")\n",
    "print(export_text(best_tree, feature_names=feature_names))\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict a new sample\n",
    "# ----------------------------\n",
    "sample = np.array([[5.9, 3.0, 5.1, 1.8]])\n",
    "pred_class = best_tree.predict(sample)[0]\n",
    "pred_prob = best_tree.predict_proba(sample)[0]\n",
    "\n",
    "print(\"\\n=== New sample prediction ===\")\n",
    "print(\"Sample:\", sample[0])\n",
    "print(\"Predicted class:\", class_names[pred_class])\n",
    "print(\"Probabilities:\", dict(zip(class_names, np.round(pred_prob, 4))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace29b1b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262accf7",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6f7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CV results (on TRAIN only) ===\n",
      "max_depth=None  mean_CV_acc=0.9644\n",
      "max_depth=   1  mean_CV_acc=0.9281\n",
      "max_depth=   2  mean_CV_acc=0.9466\n",
      "max_depth=   3  mean_CV_acc=0.9557\n",
      "max_depth=   4  mean_CV_acc=0.9644\n",
      "max_depth=   5  mean_CV_acc=0.9644\n",
      "max_depth=   6  mean_CV_acc=0.9644\n",
      "max_depth=   8  mean_CV_acc=0.9644\n",
      "max_depth=  10  mean_CV_acc=0.9644\n",
      "\n",
      "Best max_depth = None  (mean CV acc = 0.9644)\n",
      "\n",
      "=== Test results ===\n",
      "Test accuracy: 0.9737\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[13  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  0 12]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      setosa     1.0000    1.0000    1.0000        13\n",
      "  versicolor     1.0000    0.9231    0.9600        13\n",
      "   virginica     0.9231    1.0000    0.9600        12\n",
      "\n",
      "    accuracy                         0.9737        38\n",
      "   macro avg     0.9744    0.9744    0.9733        38\n",
      "weighted avg     0.9757    0.9737    0.9737        38\n",
      "\n",
      "\n",
      "=== Feature importances (higher = more important) ===\n",
      "petal length (cm)         0.4542\n",
      "petal width (cm)          0.4037\n",
      "sepal length (cm)         0.1114\n",
      "sepal width (cm)          0.0307\n",
      "\n",
      "=== New sample prediction ===\n",
      "Sample: [5.9 3.  5.1 1.8]\n",
      "Predicted class: virginica\n",
      "Probabilities: {np.str_('setosa'): np.float64(0.0), np.str_('versicolor'): np.float64(0.1233), np.str_('virginica'): np.float64(0.8767)}\n"
     ]
    }
   ],
   "source": [
    "# Random Forest on Iris with CV to choose best max_depth\n",
    "# - Step 1: CV on training set to pick best depth\n",
    "# - Step 2: Fit best RF on full training set\n",
    "# - Step 3: Evaluate on untouched test set + predict sample\n",
    "# - Also prints feature importances\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Load Iris\n",
    "# ----------------------------\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Train/Test split (keep test untouched)\n",
    "# ----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=7, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 3) CV to choose best max_depth (on TRAIN only)\n",
    "# ----------------------------\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "depth_list = [None, 1, 2, 3, 4, 5, 6, 8, 10]\n",
    "mean_scores = []\n",
    "\n",
    "for d in depth_list:\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=d,\n",
    "        random_state=7,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    scores = cross_val_score(rf, X_train, y_train, cv=cv, scoring=\"accuracy\")\n",
    "    mean_scores.append(scores.mean())\n",
    "\n",
    "best_idx = int(np.argmax(mean_scores))\n",
    "best_depth = depth_list[best_idx]\n",
    "best_cv_acc = mean_scores[best_idx]\n",
    "\n",
    "print(\"=== CV results (on TRAIN only) ===\")\n",
    "for d, m in zip(depth_list, mean_scores):\n",
    "    d_text = \"None\" if d is None else str(d)\n",
    "    print(f\"max_depth={d_text:>4s}  mean_CV_acc={m:.4f}\")\n",
    "print(f\"\\nBest max_depth = {('None' if best_depth is None else best_depth)}  (mean CV acc = {best_cv_acc:.4f})\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Train best Random Forest on full training set\n",
    "# ----------------------------\n",
    "best_rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=best_depth,\n",
    "    random_state=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Evaluate on test set\n",
    "# ----------------------------\n",
    "y_pred = best_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== Test results ===\")\n",
    "print(\"Test accuracy:\", round(acc, 4))\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "print(\"\\nReport:\\n\", classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Feature importances\n",
    "# ----------------------------\n",
    "importances = best_rf.feature_importances_\n",
    "order = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"\\n=== Feature importances (higher = more important) ===\")\n",
    "for idx in order:\n",
    "    print(f\"{feature_names[idx]:25s} {importances[idx]:.4f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Predict a new sample\n",
    "# ----------------------------\n",
    "sample = np.array([[5.9, 3.0, 5.1, 1.8]])\n",
    "pred_class = best_rf.predict(sample)[0]\n",
    "pred_prob = best_rf.predict_proba(sample)[0]\n",
    "\n",
    "print(\"\\n=== New sample prediction ===\")\n",
    "print(\"Sample:\", sample[0])\n",
    "print(\"Predicted class:\", class_names[pred_class])\n",
    "print(\"Probabilities:\", dict(zip(class_names, np.round(pred_prob, 4))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a3ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
