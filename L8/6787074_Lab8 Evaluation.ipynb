{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b5ce192",
   "metadata": {},
   "source": [
    "# Machine Learning Lab Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77af0079",
   "metadata": {},
   "source": [
    "This lab includes two tasks: regression and classification. Each task compares three models: two traditional machine learning models and one neural network, with evaluations based on three metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af8a2fdf-b6c6-4d61-80ab-f74d286a27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afccb6",
   "metadata": {},
   "source": [
    "## Task 1: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e4a7e-d263-42cb-abc6-4237a3ca66c8",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset\n",
    "We use the Boston Housing dataset for the regression task.\n",
    "\n",
    "Use Numpy library to import dataset.\n",
    "<br>`numpy.load(FILE_PATH, allow_pickle=True).item()`\n",
    "<br>The dataset structure is dictionary-format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7637af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "df = np.load(\"boston-dataset.npy\", allow_pickle=True).item()\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e06893ce-b3cc-4cca-be4d-221381d74028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 3 samples of dataset\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "pd.DataFrame(df['data'], columns=df['feature_names']).head(3)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d5c7",
   "metadata": {},
   "source": [
    "### Step 2: Split and Scale Data\n",
    "We split the dataset into training and testing sets, then standardize the features for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "16c08fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using train_test_split function\n",
    "# 80% of the data will be used for training and 20% for testing\n",
    "# Random state is set to 42\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "X = df['data']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "# -------------------------------\n",
    "\n",
    "# Standardize the data\n",
    "# Use StandardScaler to standardize the data\n",
    "# Use fit_transform method on the training data and transform method on the testing data\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "116f2833-36aa-4835-95d9-56fdeacdfcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable shape\n",
      "X_train >> (404, 13)\n",
      "X_test >> (102, 13)\n",
      "y_train >> (404,)\n",
      "y_test >> (102,)\n"
     ]
    }
   ],
   "source": [
    "# Show variable shape\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "print(\"Variable shape\")\n",
    "print(f\"X_train >> {X_train.shape}\")\n",
    "print(f\"X_test >> {X_test.shape}\")\n",
    "print(f\"y_train >> {y_train.shape}\")\n",
    "print(f\"y_test >> {y_test.shape}\")\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23535d4e",
   "metadata": {},
   "source": [
    "### Step 3: Train Models\n",
    "We train three models: Linear Regression, Random Forest Regressor, and a Neural Network.\n",
    "<br>Steps: \n",
    "1) Import/Build the model, \n",
    "2) Fit the model, \n",
    "3) Predict the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7acb0d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.99672362, 36.02556534, 14.81694405, 25.03197915, 18.76987992,\n",
       "       23.25442929, 17.66253818, 14.34119   , 23.01320703, 20.63245597,\n",
       "       24.90850512, 18.63883645, -6.08842184, 21.75834668, 19.23922576,\n",
       "       26.19319733, 20.64773313,  5.79472718, 40.50033966, 17.61289074,\n",
       "       27.24909479, 30.06625441, 11.34179277, 24.16077616, 17.86058499,\n",
       "       15.83609765, 22.78148106, 14.57704449, 22.43626052, 19.19631835,\n",
       "       22.43383455, 25.21979081, 25.93909562, 17.70162434, 16.76911711,\n",
       "       16.95125411, 31.23340153, 20.13246729, 23.76579011, 24.6322925 ,\n",
       "       13.94204955, 32.25576301, 42.67251161, 17.32745046, 27.27618614,\n",
       "       16.99310991, 14.07009109, 25.90341861, 20.29485982, 29.95339638,\n",
       "       21.28860173, 34.34451856, 16.04739105, 26.22562412, 39.53939798,\n",
       "       22.57950697, 18.84531367, 32.72531661, 25.0673037 , 12.88628956,\n",
       "       22.68221908, 30.48287757, 31.52626806, 15.90148607, 20.22094826,\n",
       "       16.71089812, 20.52384893, 25.96356264, 30.61607978, 11.59783023,\n",
       "       20.51232627, 27.48111878, 11.01962332, 15.68096344, 23.79316251,\n",
       "        6.19929359, 21.6039073 , 41.41377225, 18.76548695,  8.87931901,\n",
       "       20.83076916, 13.25620627, 20.73963699,  9.36482222, 23.22444271,\n",
       "       31.9155003 , 19.10228271, 25.51579303, 29.04256769, 20.14358566,\n",
       "       25.5859787 ,  5.70159447, 20.09474756, 14.95069156, 12.50395648,\n",
       "       20.72635294, 24.73957161, -0.164237  , 13.68486682, 16.18359697,\n",
       "       22.27621999, 24.47902364])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Regression\n",
    "# Create a Linear Regression model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_model =LinearRegression()\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_model.fit(X_train, y_train)\n",
    "# -------------------------------\n",
    "\n",
    "\n",
    "# Predict the target values\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_model.predict(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "35a80884-3f71-45b2-b5e4-d2bc45de8444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "# Use RandomForestRegressor to train the model with random_state=42\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "RandomForestRegressor(random_state=42)\n",
    "# -------------------------------\n",
    "\n",
    "# Fit the model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "rf_pred = RandomForestRegressor(random_state=42).fit(X_train, y_train)\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the target values\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "rf_pred_y_pred = rf_pred.predict(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cf3e265b-9737-4340-84ba-3c56c828a80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 530.9550   \n",
      "Epoch 2/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 377.0482 \n",
      "Epoch 3/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 184.9250 \n",
      "Epoch 4/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.8853  \n",
      "Epoch 5/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46.1462 \n",
      "Epoch 6/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30.9502 \n",
      "Epoch 7/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25.3900 \n",
      "Epoch 8/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 22.8636 \n",
      "Epoch 9/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 21.0395 \n",
      "Epoch 10/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 19.7431 \n",
      "Epoch 11/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 18.5394\n",
      "Epoch 12/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 17.5406  \n",
      "Epoch 13/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 16.6336 \n",
      "Epoch 14/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.8028\n",
      "Epoch 15/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 15.2641\n",
      "Epoch 16/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.7758 \n",
      "Epoch 17/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 14.2045 \n",
      "Epoch 18/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.8561\n",
      "Epoch 19/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 13.4885\n",
      "Epoch 20/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.9594\n",
      "Epoch 21/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.8344\n",
      "Epoch 22/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.3051 \n",
      "Epoch 23/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.2924 \n",
      "Epoch 24/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 12.0788\n",
      "Epoch 25/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.5611 \n",
      "Epoch 26/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.4767\n",
      "Epoch 27/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.6567\n",
      "Epoch 28/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 11.2832\n",
      "Epoch 29/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.8766 \n",
      "Epoch 30/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.6884\n",
      "Epoch 31/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.5546\n",
      "Epoch 32/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.3562\n",
      "Epoch 33/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.4432 \n",
      "Epoch 34/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.2962\n",
      "Epoch 35/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 10.1756 \n",
      "Epoch 36/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.9882 \n",
      "Epoch 37/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.8886 \n",
      "Epoch 38/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.7298 \n",
      "Epoch 39/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.6579 \n",
      "Epoch 40/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.5726 \n",
      "Epoch 41/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.3004 \n",
      "Epoch 42/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2978  \n",
      "Epoch 43/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2347  \n",
      "Epoch 44/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 9.2133  \n",
      "Epoch 45/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9379  \n",
      "Epoch 46/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.9051  \n",
      "Epoch 47/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.6896 \n",
      "Epoch 48/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.7099 \n",
      "Epoch 49/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.5536 \n",
      "Epoch 50/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 8.4956 \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "# Create a neural network model with 2 hidden layers with 64 units and ReLU activation function\n",
    "\n",
    "nn_model_reg = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model with adam optimizer and mean squared error loss function\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "nn_model_reg.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# -------------------------------\n",
    "\n",
    "# Fit the model with 50 epochs and batch size of 16\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "nn_model_reg.fit(X_train, y_train, epochs=50, batch_size=16)\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the target values using the model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "y_pred_nurual = nn_model_reg.predict(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce84f71",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate Models\n",
    "We evaluate the models using Mean Squared Error (MSE), Mean Absolute Error (MAE), and R² Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "475edf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Task\n",
      "Linear Regression\n",
      "MSE: 11.738288514233469\n",
      "RMSE: 3.4261185785424106\n",
      "MAE: 2.243083157258875\n",
      "R2 Score: 0.8399334112005223\n",
      "Random Forest Regressor:\n",
      "MSE: 7.84278832352941\n",
      "RMSE: 2.80049787065254\n",
      "MAE: 2.0359117647058826\n",
      "R2 Score: 0.8399334112005223\n",
      "Neural Network:\n",
      "Linear Regression\n",
      "MSE: 12.584291858589895\n",
      "RMSE: 3.5474345460614063\n",
      "MAE: 2.390969768224978\n",
      "R2 Score: 0.8399334112005223\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = (mean_squared_error(y_test, y_pred))\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "rf_pred_mae = mean_absolute_error(y_test, rf_pred_y_pred)\n",
    "rf_pred_mse = (mean_squared_error(y_test, rf_pred_y_pred))\n",
    "rf_pred_rmse = (np.sqrt(mean_squared_error(y_test, rf_pred_y_pred)))\n",
    "rf_pred_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "nurual_mae = mean_absolute_error(y_test, y_pred_nurual)\n",
    "nurual_mse = (mean_squared_error(y_test, y_pred_nurual))\n",
    "nurual_rmse = (np.sqrt(mean_squared_error(y_test, y_pred_nurual)))\n",
    "nurual_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "print(\"Regression Task\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"MSE:\", mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"R2 Score:\", r2)\n",
    "# -------------------------------\n",
    "print(\"Random Forest Regressor:\")\n",
    "print(\"MSE:\", rf_pred_mse)\n",
    "print(\"RMSE:\", rf_pred_rmse)\n",
    "print(\"MAE:\", rf_pred_mae)\n",
    "print(\"R2 Score:\", rf_pred_r2)\n",
    "# -------------------------------\n",
    "print(\"Neural Network:\")\n",
    "print(\"Linear Regression\")\n",
    "print(\"MSE:\", nurual_mse)\n",
    "print(\"RMSE:\", nurual_rmse)\n",
    "print(\"MAE:\", nurual_mae)\n",
    "print(\"R2 Score:\", nurual_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f848fdbb",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf18a44",
   "metadata": {},
   "source": [
    "## Task 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857fc80",
   "metadata": {},
   "source": [
    "### Step 1: Load Dataset\n",
    "We use the Iris dataset for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "73f88262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0           5.1          3.5           1.4          0.2       0\n",
       "1           4.9          3.0           1.4          0.2       0\n",
       "2           4.7          3.2           1.3          0.2       0\n",
       "3           4.6          3.1           1.5          0.2       0\n",
       "4           5.0          3.6           1.4          0.2       0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "CSV_PATH = \"iris_dataset.csv\"\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# -------------------------------\n",
    "\n",
    "# Show the first 5 rows of the dataset\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "df.head()\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "15f678e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "X = df.drop(['target'],axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45ad3b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]]\n",
      "\n",
      "Target\n",
      "[0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Show 3 samples of features\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "print(\"Features\")\n",
    "print(X[:3].values)\n",
    "print()\n",
    "print(\"Target\")\n",
    "print(y[:3].values)\n",
    "\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483bf4e5",
   "metadata": {},
   "source": [
    "### Step 2: Split and Scale Data\n",
    "We split the dataset into training and testing sets, then standardize the features for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b7de1a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data using train_test_split function\n",
    "# 80% of the data will be used for training and 20% for testing\n",
    "# Random state is set to 42\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "# -------------------------------\n",
    "\n",
    "# Standardize the data\n",
    "# Use StandardScaler to standardize the data\n",
    "# Use fit_transform method on the training data and transform method on the testing data\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d81ee",
   "metadata": {},
   "source": [
    "### Step 2.5: Encode Target Variable\n",
    "We encode the target variable using One-Hot Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "747b44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode for neural network\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "# Your code here\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "# -------------------------------\n",
    "\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "88a50f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable shape\n",
      "X_train >> (120, 4)\n",
      "X_test >> (30, 4)\n",
      "y_train >> (120,)\n",
      "y_test >> (30,)\n"
     ]
    }
   ],
   "source": [
    "# Show variable shape\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "print(\"Variable shape\")\n",
    "print(f\"X_train >> {X_train.shape}\")\n",
    "print(f\"X_test >> {X_test.shape}\")\n",
    "print(f\"y_train >> {y_train.shape}\")\n",
    "print(f\"y_test >> {y_test.shape}\")\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4d615",
   "metadata": {},
   "source": [
    "### Step 3: Train Models\n",
    "We train three models: Logistic Regression, Random Forest Classifier, and a Neural Network.\n",
    "<br>Steps: \n",
    "1) Import/Build the model, \n",
    "2) Fit the model, \n",
    "3) Predict the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9bf9acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "# Create a Logistic Regression model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_model = LogisticRegression()\n",
    "# -------------------------------\n",
    "\n",
    "# Fit the model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_model.fit(X_train, y_train)\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the target values\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "# -------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7deea182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "# Use RandomForestClassifier to train the model with random_state=42\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "# -------------------------------\n",
    "\n",
    "# Fit the model\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "rf_model.fit(X_train, y_train)\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the target values\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d37e7459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "# Create a neural network model with 2 hidden layers with 64 units and ReLU activation function\n",
    "nn_model_cls = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train_onehot.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with adam optimizer and categorical crossentropy loss function\n",
    "# Use accuracy as the metric\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "nn_model_cls.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# -------------------------------\n",
    "\n",
    "# Fit the model with 50 epochs and batch size of 16\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "nn_model_cls.fit(X_train, y_train_onehot, epochs=50, batch_size=16, verbose=0)\n",
    "# -------------------------------\n",
    "\n",
    "# Predict the target values using the model\n",
    "# Use argmax to get the predicted class\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "nn_pred = np.argmax(nn_model_cls.predict(X_test), axis=1)\n",
    "# -------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c8642",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate Models\n",
    "We evaluate the models using Accuracy, Precision, and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7266fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Task:\n",
      "Logistic Regression:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "\n",
      " Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Random Forest Classifier:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "\n",
      " Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n",
      "Neural Network:\n",
      "  Accuracy: 1.0000\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1 Score: 1.0000\n",
      "\n",
      " Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "# -------------------------------\n",
    "# Your code here\n",
    "print(\"Classification Task:\")\n",
    "# Logistic Regression\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, lr_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, lr_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score( y_test, lr_pred, average='weighted'):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, lr_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n Confusion Matrix:\\n{confusion_matrix(y_test, lr_pred)}\\n\")\n",
    "# Random Forest\n",
    "print(\"Random Forest Classifier:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, rf_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n Confusion Matrix:\\n{confusion_matrix(y_test, rf_pred)}\\n\")\n",
    "# Neural Network\n",
    "print(\"Neural Network:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, nn_pred):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, nn_pred, average='weighted'):.4f}\")\n",
    "print(f\"  Recall: {recall_score(y_test, nn_pred, average='weighted'):.4f}\")\n",
    "print(f\"  F1 Score: {f1_score(y_test, nn_pred, average='weighted'):.4f}\")\n",
    "print(f\"\\n Confusion Matrix:\\n{confusion_matrix(y_test, nn_pred)}\\n\")\n",
    "# -------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
